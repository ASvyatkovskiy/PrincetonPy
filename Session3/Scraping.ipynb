{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose we want to crawl the list of \"Available Technologies\" being licensed by MIT at http://technology.mit.edu and store their basic info; their associated patents; and the reference counts on their associated patents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Understanding the URL\n",
    "\n",
    "Okay, let's open Chrome browser and go to that URL.\n",
    "\n",
    "<img src=\"images/mit0.png\">\n",
    "\n",
    "- _First try_:  Aha, a list of links on the right.  Let's click on a few -- what do we see?  Many are empty, the categories are not obviously mutually exclusive, okay.  Maybe there's a better way.\n",
    "- _Second try_: Let's just search for all technologies at http://technology.mit.edu/technologies.  Okay, better but it only gives us 50 at a time.  We could just combine the four pages, that's fine.  Let's just click on page 2 to see what happens\n",
    "- _Third try_: Aha, the URL for page 2 is http://technology.mit.edu/technologies?limit=50&offset=50&query=.  That looks like we can just specify a higher limit and offset 0 and get the whole thing.\n",
    "- _Final answer_: Indeed, http://technology.mit.edu/technologies?limit=1000 has a giant list.\n",
    "\n",
    "\n",
    "We are going to seu `urllib2` python library to get the raw page content as a string. Note: this library is not available in Python3, instead `urllib` should be used, which has almost identical API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "\n",
    "url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "raw_page = urllib2.urlopen(url).read()\n",
    "#print raw_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: CSS selectors\n",
    "\n",
    "Let us inspect the http://technology.mit.edu/technologies?limit=1000 in Chrome.\n",
    "Open View->Developer->Developer Tools. Or in the newer version simply: however over the element, right click->Inspect\n",
    "Right click on one of the technology titles, and choose \"Inspect Element\".\n",
    "\n",
    "<img src=\"images/mit1.png\">\n",
    "\n",
    "What are we looking at? Well.. it's this is the structure of the webpage. Nested tags of different types and having a variety of attributes.\n",
    "\n",
    "  - All of the technologies are underneath (\"_descendents of_\")   `<div class=\"search\" id=\"nouvant-portfolio-content\">`\n",
    "  - In fact, each of them is in its own `<div class=\"technology\" data-images=\"true\" id=\"technology_XXXX\">`\n",
    "    \n",
    "Now we're ready to move on: we'll use BeautifulSoup to leverage the above to zoom in on the individual technologies and to get links to the pages with detailed info.\n",
    "\n",
    "This pattern -- where you have nested finds, each given by conditions on tag type, id, and class -- is very common.  It's so common that there is a special convenience language for such traversals: [CSS selectors](http://www.w3schools.com/cssref/css_selectors.asp).\n",
    "\n",
    "BeautifulSoup supports a form of CSS selectors, and this will let us write the above in a more concise and expressive way:\n",
    "    >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "All selectors work like a 'find_all'.  Some basic building examples of selectors are:\n",
    "\n",
    " - _'mytag'_ picks out all tags of type _mytag_.\n",
    " - _'#myid'_ picks out all tags whose _id_ is equal to _myid_\n",
    " - _'.myclass'_ picks out all tags whose _class_ is equal to _myclass_\n",
    " - _'mytag#myid'_ will pick all tags of type _mytag_ **and** _id_ equal to _myid_ (analgously for _'mytag.myclass'_)\n",
    " - If _'selector1'_ and _'selector2'_ are two selectors, then there is another selector '_selector1 selector2'_.  It picks out all tags satisfying _selector2_ that are __descendents__(*) of something satisfying _selector1_, i.e., it's like our nested find.\n",
    " \n",
    " (*) It doesn't have to be a _direct_ descedent.  I.e., it can be a grand-grand-..-grand-child of something satisfying _selector1_.  For direct descendents we'd instead write _'selector1 > selector2'_\n",
    " \n",
    "Let's just explain how this applies to our example:\n",
    "\n",
    "1.  Let's start with the first half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with id 'nouvant-portfolio-content'.\n",
    "2.  Then the second half\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                                                            ^^^^^^^^^^^^^^\n",
    "This picks out all 'div' tags with class 'technology'.\n",
    "3.  Finally the whole thing\n",
    "        >    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "        >                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "does exactly the same as our nested find above!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(raw_page)\n",
    "#print soup.prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following a detailed description above, we will first find all tags of interested using 2 `find` statements, and then using one nested `select` statement from BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "parent_div = soup.find('div', attrs={'id': 'nouvant-portfolio-content'}) #Find (at most) *one*\n",
    "tech_divs = parent_div.find_all('div', attrs={'class':'technology'})  #Find *all*\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two find statements can be combined into one select statement with a more complex CSS pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "tech_divs = soup.select('div#nouvant-portfolio-content div.technology')\n",
    "print len(tech_divs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what we've zoomed in to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"technology\" data-images=\"false\" id=\"technology_10355\">\n",
      " <h2>\n",
      "  <a href=\"/technologies/17704_procedural-sedation-monitoring-systems-and-methods-for-predicting-adverse-events-and-assessing-level-of-sedation\">\n",
      "   Procedural Sedation Monitoring: Systems and Methods for Predicting Adverse Events and Assessing Level of Sedation\n",
      "  </a>\n",
      " </h2>\n",
      " <p>\n",
      "  <strong>\n",
      "   17704\n",
      "  </strong>\n",
      "  –\n",
      "  <span>\n",
      "   This invention is a system for monitoring sedation state and detecting adverse events during procedural sedation. The system can be implemented in a standalone monitor or incorporated into commercially available monitoring systems within clinical settings.     Procedural sedation has allowed many painful procedures to be conducted outside the operating room. During such procedures, it is...\n",
      "   <a href=\"/technologies/17704_procedural-sedation-monitoring-systems-and-methods-for-predicting-adverse-events-and-assessing-level-of-sedation\">\n",
      "    Read More\n",
      "   </a>\n",
      "  </span>\n",
      " </p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print tech_divs[0].prettify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to pull out some key pieces of info:\n",
    "\n",
    "- The technology's \"title\" (the text in the `<a>` element)\n",
    "- The link to follow for more info on the technology (the _href_ attribute of the `<a>`)\n",
    "- And a short blurb about the text (in the `<span>`)\n",
    "\n",
    "Let's write some code to extract this.  But before we do, let's discuss what _form_ the output should take: It is often convenient to store data in _key-value_ form (e.g., as a hashtable), in other words to name the bits of data you are collecting.  One big advantage is that this makes it easier to add in extra fields progrssively.\n",
    "\n",
    "Let's see what the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedural Sedation Monitoring: Systems and Methods for Predicting Adverse Events and Assessing Level of Sedation \n",
      "/technologies/17704_procedural-sedation-monitoring-systems-and-methods-for-predicting-adverse-events-and-assessing-level-of-sedation\n"
     ]
    }
   ],
   "source": [
    "firsta = tech_divs[0].find('a')\n",
    "print firsta.text\n",
    "print firsta['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a \"named tuple\" to store our key-value data. We could also have used a dictionary, with strings as keys.\n",
    "\n",
    "Named tuples have some advantages\n",
    " - Better notation, `x.field_name instead of x['field_name']`\n",
    " - If you change your object structure later and fail to update your code to include the new fields, this will make it easier to find.\n",
    " - They are immutable, preventing certain sorts of bugs... and some disadvantages:\n",
    "   - If you want to augment object structure you need a new type (or to go back and fill your code )\n",
    "   - They are immutable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/technologies/17704_procedural-sedation-monitoring-systems-and-methods-for-predicting-adverse-events-and-assessing-level-of-sedation\n",
      "    This invention is a system for monitoring sedation state and detecting adverse events during procedural sedation. The system can be implemented in a standalone monitor or incorporated into commercially available monitoring systems within clinical settings.     Procedural sedation has allowed many painful procedures to be conducted outside the operating room. During such procedures, it is... Read More\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "\n",
    "def td_info(td):\n",
    "    la = td.select('h2 > a')\n",
    "    ls = td.select('span')\n",
    "    if len(la)!=1 or len(ls)!=1:\n",
    "        print \"Uh oh! We did something wrong\"\n",
    "        return None\n",
    "    return TechBasic (\n",
    "            title = la[0].text,\n",
    "            url   = la[0]['href'],\n",
    "            short = ls[0].text\n",
    "            )\n",
    "tech_links=[td_info(td) for td in tech_divs]\n",
    "\n",
    "print tech_links[0].url\n",
    "print tech_links[0].short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the patent information\n",
    "\n",
    "Let us start from the end. First, I will provide the solution to the problem, and then we will work backwards to understand it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TechDetailed(tech_basic=TechBasic(title=u'The Visual Microphone', url=u'/technologies/16488_the-visual-microphone', short=u' \\n\\n   \\n\\n The inventors have\\ndeveloped a method to turn effectively any object into a visual microphone to\\nenable the detection of sound from afar. Sound signals produce air pressure\\nfluctuations that cause objects in the vicinity to vibrate. This method works\\nby analyzing video recordings of these vibrations to convert them back into a\\ncorresponding sound signal. This technology is mainly... Read More'), patents=[Patent(name=u'US Patent 2015-0319540', url=u'http://google.com/patents/US20150319540')]), TechDetailed(tech_basic=TechBasic(title=u'Methods of Evaluating Gene Expression Levels', url=u'/technologies/15194_methods-of-evaluating-gene-expression-levels', short=u'    The ability to build a higher level representation of a novel biological design from known parts, with flexible protocol automation and DNA expression characterization, is useful in any industrial or academic setting where accurate predictions of the protein expression output of a genetic circuit is relevant to the project design.    The ability to accurately predict the expression levels... Read More'), patents=[Patent(name=u'US Patent 8,809,057', url=u'http://google.com/patents/US8809057')])]\n"
     ]
    }
   ],
   "source": [
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "def get_tech_details(tech_basic):\n",
    "    url_base=\"http://technology.mit.edu/\"\n",
    "    soup = BeautifulSoup( urllib2.urlopen(url_base + tech_basic.url) )\n",
    "    def patent_info(a):\n",
    "       return Patent ( \n",
    "                name = a.text, \n",
    "                url = a['href'] \n",
    "                )\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed ( \n",
    "            tech_basic = tech_basic, \n",
    "            patents = patents \n",
    "            )\n",
    "\n",
    "tech_basics = map(get_tech_details, tech_links[0:10])  #This takes a list\n",
    "\n",
    "#okay, some technologies will not have associated patents. So let us filter to keep only those that have\n",
    "print filter(lambda x: len(x.patents) != 0, tech_basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "In the last code segment, we only did the first one.  If we try to get them all this way, it'll take a while.  Run the next cell for as long (or not) as you wish, and when you get bored use _Kernel->Interrupt_ to stop it.\n",
    "\n",
    "The problem is of course that it takes a while to connect to the remote server and fetch the page.  Fortunately, thought it takes a long time it is not actually _computationally expensive_: your computer would be perfectly happy doing this for 20 pages at a time.  The **multiprocessing** package in Python makes it easy to do this kind of (easy) parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Slow version\n",
    "# Uncomment and run it to see\n",
    "# import time\n",
    "\n",
    "# start_time = time.time()\n",
    "# tech_details = map(get_tech_details, tech_links)  #This takes a list\n",
    "# end_time = time.time()\n",
    "\n",
    "# print \"Done!\", end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 4 cores...\n",
      "Done! 23.4673888683\n"
     ]
    }
   ],
   "source": [
    "# Multi-processor version\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "cpu_count=mp.cpu_count()\n",
    "workers = Pool(cpu_count-1) \n",
    "\n",
    "print \"Running with {} cores...\".format(cpu_count)\n",
    "\n",
    "start_time = time.time()\n",
    "tech_details = workers.map(get_tech_details, tech_links)\n",
    "end_time = time.time()\n",
    "\n",
    "print \"Done!\", end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "\n",
    "Let's put all of that together.  Write a function \n",
    "```python\n",
    "def get_tech_basics(url):\n",
    "    ...\n",
    "```\n",
    "\n",
    "that returns `TechBasic` all each technology on the page.  Combine this with the pooled requests to get_tech_details to obtain a list of TechDetails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin.**\n",
    "That's it, we now have a basic not-entirely-trivial example.  Along the way we took some detours, so let's just take a look at what our code looks like without those detours:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises:**\n",
    "\n",
    "1. Modify \"get_tech_details\" to get other interesting information on the technology, like a long form description and/or the authors' names.  (You'll also want to modify TechDetailed.  Do that first and note that now the code breaks when it tries to construct a TechDetailed with the wrong number of fields.)\n",
    "\n",
    "2. Modify \"get_tech_details\" to try to follow the link and to get more information on the patent -- for instance when it was filed and granted, or how many other patents reference it.  (Warning: The patent web site is much less regular than MIT's!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import namedtuple\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "\n",
    "cpu_count=mp.cpu_count()\n",
    "\n",
    "# Getting the list of short 'blurbs' about the techs\n",
    "TechBasic = namedtuple('TechBasic', 'title, url, short')\n",
    "def get_tech_basics(url):\n",
    "    url = \"http://technology.mit.edu/technologies?limit=1000\"\n",
    "    soup = BeautifulSoup(urllib2.urlopen(url))\n",
    "\n",
    "    ## Get the list of tech blurbs\n",
    "    tech_divs = soup.select('div#nouvant-portfolio-content  div.technology')\n",
    "\n",
    "    ## Parse a single 'td' on the index page\n",
    "    def td_info(td):\n",
    "        la = td.select('h2 > a')\n",
    "        ls = td.select('span')\n",
    "        if len(la)!=1 or len(ls)!=1:\n",
    "            print \"Uh oh! We did something wrong\"\n",
    "            return None\n",
    "        return TechBasic (\n",
    "                title = la[0].text,\n",
    "                url   = la[0]['href'],\n",
    "                short = ls[0].text\n",
    "                )\n",
    "    \n",
    "    return [td_info(td) for td in tech_divs]\n",
    "\n",
    "\n",
    "# Adding in some details (just patent info, for now)\n",
    "Patent = namedtuple('Patent', 'name url')\n",
    "TechDetailed = namedtuple('TechDetailed', 'tech_basic, patents')\n",
    "def get_tech_details(tech_basic):\n",
    "    url_base=\"http://technology.mit.edu/\"\n",
    "    soup = BeautifulSoup( urllib2.urlopen(url_base + tech_basic.url) )\n",
    "    def patent_info(a):\n",
    "       return Patent ( \n",
    "                name = a.text, \n",
    "                url = a['href'] \n",
    "                )\n",
    "    patents = [patent_info(a) for a in soup.select('dd.us_patent_issued a')]\n",
    "    return TechDetailed ( \n",
    "            tech_basic = tech_basic, \n",
    "            patents = patents \n",
    "            )\n",
    "\n",
    "## The main driver code:\n",
    "tech_basics = get_tech_basics(\"http://technology.mit.edu/technologies?limit=1000\")\n",
    "\n",
    "workers = Pool(cpu_count)  # number of worker processes\n",
    "tech_details = workers.map(get_tech_details, tech_basics)\n",
    "\n",
    "print tech_details[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example: accessing info from a web page through drop-down menu\n",
    "\n",
    "\n",
    "Suppose we need to collect the text information for different reports on the **Fed Board Governors** Webpage and output it in a table if possible.\n",
    "\n",
    "URL: http://www.federalreserve.gov/apps/reportforms/default.aspx\n",
    "\n",
    "<img src=\"images/dropdown1.png\">\n",
    "\n",
    "For every form, we need to collect the basic information: \n",
    " - Name of Form\n",
    " - Description\n",
    " - OMB\n",
    " - Purpose\n",
    " - Background\n",
    " - Respondent Panel\n",
    " - Frequency\n",
    " \n",
    "Not every form will have all this information, but whatever is there we’d like to scrape. We do not need to download the forms, just want the information, the whole block of text is fine.\n",
    "\n",
    "<img src=\"images/dropdown2.png\">\n",
    "\n",
    "In the end, we would like to structure the information as a CSV file:\n",
    "\n",
    "**Form Number; Description; OMB; Purpose; Background; Respondent Panel; Frequency; Public Release; FR 2004**\n",
    "\n",
    "Inspecting the URL and playing with it a bit, we find out that there is no URL we could derive, that would allow to capture all the information we need. It looks like we would need to start from the front page and navigate through it, select something from a drop-down menu like a human would!\n",
    "\n",
    "While we can still use CSS selectors to find web elements on the page, BeautifulSoup would not allow us to navigate through the page easily (i.e. click buttons) or invoke embeded javascript. However, Selenium library can allow us all do that!\n",
    "\n",
    "### Selenium\n",
    "\n",
    "Selenium WebDriver API supports different possibilities to identify elements: by ID, by CLASS, by NAME, by CSS selector, by XPath, by TAG name. \n",
    "\n",
    "To inspect an element you just have to open the desired web page, right-click the desired element and click on Inspect Element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up pyvirtualdisplay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "#Display - read about pyvirtualdisplay\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "#webdriver - read about selenium.webdriver\n",
    "driver = webdriver.Firefox()\n",
    "    \n",
    "#this is a starting page we are scraping\n",
    "driver.get(\"http://www.federalreserve.gov/apps/reportforms/default.aspx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every element on the HTML page can be located using CSS selectors (similar to the previous problem discussed).\n",
    "Opening the starting page in Chrome or Firefox, right click on the drop-down menu, click \"Inspect\" we see a tag on the right highlighted, we copy it's id - `MainContent_ddl_ReportForms`.\n",
    "\n",
    "Knowing the id of dropdown menu, we can locate it with Selenium like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_menu = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"#MainContent_ddl_ReportForms\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop down menu is an HTML table of options which can be verified in Chrome browser (Developer Tools, that pop up when you right click and press \"Inspect\" on an element)\n",
    "\n",
    "Following returns all of the options - rows in that table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "form_options = main_menu.find_elements_by_tag_name(\"option\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "#We count them\n",
    "option_count = len(form_options)\n",
    "print option_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate through the menu - essentially, like we scrolling down the drop down menu and clicking on each every form!\n",
    "\n",
    "Here is an example for the first item only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFIEC 001\n"
     ]
    }
   ],
   "source": [
    "#list to store all scraped data\n",
    "all_items = list()\n",
    "\n",
    "\n",
    "#Get web element corresponding to a form\n",
    "form = form_options[1]\n",
    "#Click as a mouse click-action in browser \n",
    "form.click()\n",
    "#Get text, because we need to store the form number\n",
    "form_id = form.text\n",
    "print form_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Locate a web element corresponding to the submit button. By CSS selector which we found by inspection in Chrome browser (same logic as above)\n",
    "submit_button = WebDriverWait(driver,3).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"#MainContent_btn_GetForm\")))\n",
    "#Click as a mouse click-action in browser \n",
    "submit_button.click()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Board of Governors of the Federal Reserve System discontinued the Annual Report of Trust Assets (FFIEC 001; OMB No. 7100-0031), effective with the December 31, 2001, report. The Federal Reserve had collected the FFIEC 001 report from all state member banks that had been granted trust powers and from trust company subsidiaries of bank holding companies not otherwise supervised by a federal banking agency. The purpose of the report was to provide information on the volume and character of discretionary fiduciary activities exercised by such institutions.\n"
     ]
    }
   ],
   "source": [
    "#Explore all the infor we want to scrape: 'Description','OMB','Background',\n",
    "#'RespondentPanel','Frequency','PublicRelease'. some of them might nit be available on the page\n",
    "\n",
    "description = driver.find_element_by_css_selector(\"#MainContent_lbl_Description_data\") \n",
    "print description.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OMB tag is not present on this particular page... Can be coinfirmed by manually inspecting the element\n",
    "#OMB = driver.find_element_by_css_selector(\"#MainContent_lbl_OMB_data\") \n",
    "#print OMB.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now we assemble all the pieces of code above into an a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_items(max_num_items=None):\n",
    "    #list to store all scraped data\n",
    "    all_items = list()\n",
    "\n",
    "    #Display - read about pyvirtualdisplay\n",
    "    display = Display(visible=0, size=(1024, 768))\n",
    "    display.start()\n",
    "    #webdriver - read about selenium.webdriver\n",
    "    driver = webdriver.Firefox()\n",
    "    \n",
    "    #this is a starting page we are scraping\n",
    "    driver.get(\"http://www.federalreserve.gov/apps/reportforms/default.aspx\")\n",
    "    #Every element on the HTML page can be located using CSS selectors.\n",
    "    #Opening the starting page in Chrome, right click on the drop-down menu, click \"Inspect\" we see a tag on the right highlighted, we copy it's id - MainContent_ddl_ReportForms\n",
    "    #Knowing the id of dropdown menu, we can locate it with Selenium like this\n",
    "    main_menu = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"#MainContent_ddl_ReportForms\")))\n",
    "    #Drop down menu is an HTML table of options which can be verified in Chrome browser (Developer Tools, that pop up when you right click and press \"Inspect\" on an element)\n",
    "    #Following returns all of the options - rows in that table\n",
    "    form_options = main_menu.find_elements_by_tag_name(\"option\")\n",
    "    #We count them\n",
    "    option_count = len(form_options)\n",
    "    if max_num_items is not None:\n",
    "        option_count = min(max_num_items,option_count) \n",
    "        if option_count < 2: \n",
    "            print \"Need to inspect at least one option\"\n",
    "            exit(1)\n",
    "    #Next, we loop over all of them - essentially like we scrolling down the drop down menu and clicking on each every form \n",
    "    for form_i in xrange(1,option_count):\n",
    "        #Get web element corresponding to a form\n",
    "        form = form_options[form_i]\n",
    "        #Click as a mouse click-action in browser \n",
    "        form.click()\n",
    "        #Get text, because we need to store the form number\n",
    "        form_id = form.text\n",
    "        #Locate a web element corresponding to the submit button. By CSS selector which we found by inspection in Chrome browser (same logic as above)\n",
    "        submit_button = WebDriverWait(driver,3).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"#MainContent_btn_GetForm\")))\n",
    "        #Click as a mouse click-action in browser \n",
    "        submit_button.click()      \n",
    "        #Prepare data structures to store all the info we want to scrape\n",
    "        a = dict.fromkeys(['Description','OMB','Background','RespondentPanel','Frequency','PublicRelease'])\n",
    "        #We are on a web page after submit-click, following will search for all items of interest. Or for corresponding\n",
    "        #web-elements \n",
    "        for el in a.keys():\n",
    "            try:\n",
    "                item = driver.find_element_by_css_selector(\"#MainContent_lbl_\"+el+\"_data\") \n",
    "                #Once found it will store them in our dictionary, if not it will proceed to \"except\" section and do nothing\n",
    "                a[el] = item.text \n",
    "            except: \n",
    "                #case when there is no such field\n",
    "                pass\n",
    "        #we need form number as well\n",
    "        a['FormNumber'] = form_id\n",
    "        #keeping them all in one list, which will have a dictionary per Form Number - and later, a row in your excel file per Form number\n",
    "        all_items.append(a)\n",
    "    \n",
    "        #Ok, that part bothers me a little: it looks like I have to refresh \"form_options\" each time... \n",
    "        #Otherwise I get following exception: selenium.common.exceptions.StaleElementReferenceException: Message: Element not found in the cache - perhaps the page has changed since it was looked up\n",
    "        driver.get(\"http://www.federalreserve.gov/apps/reportforms/default.aspx\")\n",
    "        main_menu = WebDriverWait(driver,10).until(EC.presence_of_element_located((By.CSS_SELECTOR,\"#MainContent_ddl_ReportForms\")))\n",
    "        form_options = main_menu.find_elements_by_tag_name(\"option\")\n",
    "\n",
    "    driver.close()\n",
    "    display.stop()\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "all_items = get_all_items(2)\n",
    "print len(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'OMB': None, 'Description': None, 'RespondentPanel': None, 'FormNumber': u'FFIEC 001', 'Frequency': None, 'Background': None, 'PublicRelease': None}]\n"
     ]
    }
   ],
   "source": [
    "print all_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see all of the data is \"None\" - what went wrong? Let us check in the debugger.\n",
    "\n",
    "**Spoiler**: use WebDriverWait to wait for presence of element looked for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect what we have screaped either manually or loading into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "scraped_data = pd.read_csv(\"forms.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FormNumber</th>\n",
       "      <th>Description</th>\n",
       "      <th>OMB</th>\n",
       "      <th>Background</th>\n",
       "      <th>RespondentPanel</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>PublicRelease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FFIEC 001</td>\n",
       "      <td>The Board of Governors of the Federal Reserve ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FormNumber                                        Description  OMB  \\\n",
       "0  FFIEC 001  The Board of Governors of the Federal Reserve ...  NaN   \n",
       "\n",
       "   Background  RespondentPanel  Frequency  PublicRelease  \n",
       "0         NaN              NaN        NaN            NaN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping web pages that require username/password\n",
    "\n",
    "If you need to scrape data from a web page that requires a username/password login - for instance, a forum.\n",
    "You can use mechanize and cookielib Python libraries.\n",
    "\n",
    "For instance, following forum requires registration: http://www.mothering.com/forum/443-i-m-not-vaccinating\n",
    "\n",
    "<img src=\"images/mothering0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mechanize\n",
    "import cookielib\n",
    "\n",
    "def doLogin():\n",
    "    # Browser\n",
    "    br = mechanize.Browser()\n",
    "\n",
    "    # Cookie Jar\n",
    "    cj = cookielib.LWPCookieJar()\n",
    "    br.set_cookiejar(cj)\n",
    "\n",
    "    # Browser options\n",
    "    br.set_handle_equiv(True)\n",
    "    br.set_handle_redirect(True)\n",
    "    br.set_handle_referer(True)\n",
    "    br.set_handle_robots(False)\n",
    "    br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)\n",
    "\n",
    "    br.addheaders = [('User-agent', 'Chrome')]\n",
    "\n",
    "    # The site we will navigate into, handling its session\n",
    "    br.open('http://www.mothering.com/forum/login.php?do=login')\n",
    "\n",
    "    # View available forms\n",
    "    for f in br.forms():\n",
    "        print f\n",
    "\n",
    "    # Select the second (index one) form (the first form is a search query box)\n",
    "    br.select_form(nr=1)\n",
    "\n",
    "    # User credentials\n",
    "    br.form['vb_login_username'] = 'giant_cat'\n",
    "    br.form['vb_login_password'] = 'TestPassword33!'\n",
    "    # Login\n",
    "    br.submit()\n",
    "    return br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<GET http://www.mothering.com/forum/gtsearch.php application/x-www-form-urlencoded\n",
      "  <HiddenControl(cx=partner-pub-7865546952023728:8370985649) (readonly)>\n",
      "  <HiddenControl(cof=FORID:11) (readonly)>\n",
      "  <HiddenControl(ie=UTF-8) (readonly)>\n",
      "  <TextControl(q=)>\n",
      "  <SubmitControl(sa=Search) (readonly)>>\n",
      "<POST http://www.mothering.com/forum/login.php?do=login application/x-www-form-urlencoded\n",
      "  <TextControl(vb_login_username=User Name)>\n",
      "  <PasswordControl(vb_login_password=)>\n",
      "  <SubmitControl(<None>=Log in) (readonly)>\n",
      "  <CheckboxControl(cookieuser=[1])>\n",
      "  <HiddenControl(s=) (readonly)>\n",
      "  <HiddenControl(securitytoken=guest) (readonly)>\n",
      "  <HiddenControl(do=login) (readonly)>\n",
      "  <HiddenControl(vb_login_md5password=) (readonly)>\n",
      "  <HiddenControl(vb_login_md5password_utf=) (readonly)>>\n",
      "<POST http://www.mothering.com/forum/profile.php?do=dismissnotice application/x-www-form-urlencoded\n",
      "  <HiddenControl(do=dismissnotice) (readonly)>\n",
      "  <HiddenControl(securitytoken=guest) (readonly)>\n",
      "  <HiddenControl(dismiss_noticeid=) (readonly)>\n",
      "  <HiddenControl(url=/forum/index.php) (readonly)>>\n",
      "<GET http://www.mothering.com/forum/forumdisplay.php application/x-www-form-urlencoded\n",
      "  <HiddenControl(s=) (readonly)>\n",
      "  <HiddenControl(daysprune=) (readonly)>\n",
      "  <SelectControl(f=[cp, pm, subs, wol, search, *home, 1, 5, 17882, 2, 18090, 17898, 7, 76, 17890, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 95, 89, 90, 97, 91, 92, 93, 94, 17441, 17442, 17440, 96, 344, 300, 298, 341, 301, 299, 395, 269, 489, 212, 458, 108, 177, 18, 68, 179, 178, 180, 19, 17458, 18017, 18025, 18042, 18057, 18065, 18066, 18074, 18082, 18098, 17761, 17769, 17777, 17801, 17817, 17825, 17826, 17833, 17841, 17865, 17906, 17978, 18010, 17650, 17610, 17585, 17602, 17618, 17634, 17666, 17681, 17690, 17706, 17722, 17730, 17746, 17642, 17548, 17549, 17547, 17546, 17545, 17544, 17543, 17542, 17541, 17553, 17556, 17594, 17658, 17922, 17930, 17938, 17946, 17954, 17962, 17970, 17494, 17495, 17496, 17497, 17498, 17577, 17491, 450, 459, 460, 485, 490, 508, 511, 513, 515, 517, 519, 521, 522, 524, 525, 527, 528, 534, 538, 540, 541, 545, 547, 548, 551, 552, 553, 554, 555, 556, 563, 16169, 16170, 16266, 16464, 16778, 16935, 16962, 16974, 16999, 17490, 250, 251, 252, 254, 255, 256, 257, 258, 259, 260, 261, 262, 272, 273, 274, 291, 293, 294, 295, 296, 325, 326, 327, 338, 336, 337, 339, 340, 354, 355, 356, 357, 358, 359, 361, 362, 364, 372, 374, 383, 384, 387, 392, 394, 397, 402, 407, 415, 419, 423, 424, 427, 428, 431, 432, 435, 440, 446, 213, 16938, 16940, 16939, 16941, 166, 20, 17256, 21, 306, 502, 28, 44, 17426, 16745, 22, 156, 455, 17509, 32, 371, 27, 363, 25, 221, 227, 245, 37, 17510, 17511, 31, 17515, 310, 305, 17517, 17518, 17512, 65, 39, 43, 51, 285, 284, 286, 287, 50, 439, 36, 17516, 17513, 35, 329, 181, 303, 449, 451, 452, 211, 159, 158, 165, 333, 438, 235, 370, 157, 234, 16907, 17466, 17443, 16919, 16909, 16927, 16914, 16912, 302, 17809, 47, 18041, 373, 17507, 45, 11, 264, 317, 307, 308, 17532, 16, 12, 13, 265, 64, 253, 15, 297, 421, 447, 17514, 311, 66, 236, 312, 445, 315, 316, 10, 347, 267, 17523, 365, 366, 314, 422, 14, 220, 17738, 17994, 16874, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 18033, 16890, 16891, 16892, 16893, 16894, 16895, 17078, 110, 17539, 228, 17519, 69, 115, 112, 116, 114, 117, 113, 119, 244, 240, 241, 243, 242, 120, 111, 239, 149, 353, 504, 128, 546, 40, 281, 331, 375, 425, 248, 379, 183, 223, 448, 526, 275, 330, 279, 335, 342, 280, 16865, 16971, 17009, 16983, 16945, 501, 393, 345, 457, 106, 277, 343, 346, 225, 167, 140, 174, 172, 173, 139, 142, 401, 168, 367, 549])>\n",
      "  <SubmitControl(<None>=Go) (readonly)>>\n",
      "<GET http://www.mothering.com/forum/gtsearch.php application/x-www-form-urlencoded\n",
      "  <HiddenControl(cx=partner-pub-7865546952023728:8370985649) (readonly)>\n",
      "  <HiddenControl(cof=FORID:11) (readonly)>\n",
      "  <HiddenControl(ie=UTF-8) (readonly)>\n",
      "  <TextControl(q=)>\n",
      "  <SubmitControl(sa=Search) (readonly)>>\n",
      "<GET http://www.mothering.com/forum/ application/x-www-form-urlencoded>\n",
      "[<a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1566201-vaccination-forum-guidelines.html\" id=\"thread_title_1566201\" itemprop=\"headline\">Vaccination Forum Guidelines</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1551946-guidelines-referencing-articles-studies-another-site.html\" id=\"thread_title_1551946\" itemprop=\"headline\">Guidelines For Referencing Articles Or Studies From Another Site</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1402834-vaccinating-schedule.html\" id=\"thread_title_1402834\" itemprop=\"headline\">Vaccinating on Schedule</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1501602-baby-gets-vaccines-friday-wish-there-more-vaccines.html\" id=\"thread_title_1501602\" itemprop=\"headline\">Baby gets Vaccines on Friday, Wish there was more vaccines</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1502482-what-s-harm-people-believing-mmr-asd-link.html\" id=\"thread_title_1502482\" itemprop=\"headline\">What's the harm in people believing in a mmr/asd link?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1502138-another-study-debunking-mmr-autism-link.html\" id=\"thread_title_1502138\" itemprop=\"headline\">Another study debunking the MMR autism link</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1498802-learning-hard-way-my-journey-antivaxx-science.html\" id=\"thread_title_1498802\" itemprop=\"headline\">Learning the Hard Way: My Journey from #AntiVaxx to Science</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1500906-facts-your-friends-vaccinate-your-children.html\" id=\"thread_title_1500906\" itemprop=\"headline\">Facts Are Your Friends -- Vaccinate Your Children</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1497610-vaccine-friendly-doctors-ma.html\" id=\"thread_title_1497610\" itemprop=\"headline\">vaccine friendly doctors in Ma</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1498922-mother-s-vaccines-cause-kids-autism.html\" id=\"thread_title_1498922\" itemprop=\"headline\">Mother's Vaccines Cause Kids Autism</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1495506-myths-spread-anti-vaxers.html\" id=\"thread_title_1495506\" itemprop=\"headline\">Myths spread by anti vaxers</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1495954-emerging-data-viral-vectors-treat-cancer.html\" id=\"thread_title_1495954\" itemprop=\"headline\">Emerging Data on viral vectors to treat cancer</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1492738-how-many-shots-once.html\" id=\"thread_title_1492738\" itemprop=\"headline\">How many shots at once?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1494338-i-gave-my-child-autism.html\" id=\"thread_title_1494338\" itemprop=\"headline\">\"I Gave My Child Autism\"</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1489346-2015-2016-flu-season.html\" id=\"thread_title_1489346\" itemprop=\"headline\">2015-2016 flu season</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1488874-jimmy-kimmel.html\" id=\"thread_title_1488874\" itemprop=\"headline\">Jimmy Kimmel</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1487850-have-any-your-kids-had-reaction.html\" id=\"thread_title_1487850\" itemprop=\"headline\">Have any of your kids had a reaction?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1481946-losing-friendships-over-vaxxing.html\" id=\"thread_title_1481946\" itemprop=\"headline\">Losing friendships over vaxxing?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1486786-science-anti-vaccination.html\" id=\"thread_title_1486786\" itemprop=\"headline\">The Science of Anti-Vaccination</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1486226-ben-goldacre-vs-toronto-star.html\" id=\"thread_title_1486226\" itemprop=\"headline\">Ben Goldacre vs The Toronto Star</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1391631-article-anti-vaccination-activists-should-not-given-say-media.html\" id=\"thread_title_1391631\" itemprop=\"headline\">Article \"Anti-vaccination activists should not be given a say in the media\"  </a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1480394-do-your-research-pro-vax-blogs-journals.html\" id=\"thread_title_1480394\" itemprop=\"headline\">DO YOUR RESEARCH! Pro-vax Blogs and Journals</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1485026-chris-hayes-msnbc-oeall-ina-bob-sears.html\" id=\"thread_title_1485026\" itemprop=\"headline\">Chris Hayes MSNBC \\u201cAll In\\u201d Bob Sears</a>]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "login = doLogin()\n",
    "main_url = 'http://www.mothering.com/forum/17507-vaccinating-schedule' \n",
    "#'http://www.mothering.com/forum/443-i-m-not-vaccinating'\n",
    "page = login.open(main_url+'/index5.html').read()\n",
    "soup = BeautifulSoup(page)\n",
    "links = soup.findAll(href = re.compile(main_url+\"/\\d+(-.*)+.html$\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1566201-vaccination-forum-guidelines.html\" id=\"thread_title_1566201\" itemprop=\"headline\">Vaccination Forum Guidelines</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1551946-guidelines-referencing-articles-studies-another-site.html\" id=\"thread_title_1551946\" itemprop=\"headline\">Guidelines For Referencing Articles Or Studies From Another Site</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1402834-vaccinating-schedule.html\" id=\"thread_title_1402834\" itemprop=\"headline\">Vaccinating on Schedule</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1501602-baby-gets-vaccines-friday-wish-there-more-vaccines.html\" id=\"thread_title_1501602\" itemprop=\"headline\">Baby gets Vaccines on Friday, Wish there was more vaccines</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1502482-what-s-harm-people-believing-mmr-asd-link.html\" id=\"thread_title_1502482\" itemprop=\"headline\">What's the harm in people believing in a mmr/asd link?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1502138-another-study-debunking-mmr-autism-link.html\" id=\"thread_title_1502138\" itemprop=\"headline\">Another study debunking the MMR autism link</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1498802-learning-hard-way-my-journey-antivaxx-science.html\" id=\"thread_title_1498802\" itemprop=\"headline\">Learning the Hard Way: My Journey from #AntiVaxx to Science</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1500906-facts-your-friends-vaccinate-your-children.html\" id=\"thread_title_1500906\" itemprop=\"headline\">Facts Are Your Friends -- Vaccinate Your Children</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1497610-vaccine-friendly-doctors-ma.html\" id=\"thread_title_1497610\" itemprop=\"headline\">vaccine friendly doctors in Ma</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1498922-mother-s-vaccines-cause-kids-autism.html\" id=\"thread_title_1498922\" itemprop=\"headline\">Mother's Vaccines Cause Kids Autism</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1495506-myths-spread-anti-vaxers.html\" id=\"thread_title_1495506\" itemprop=\"headline\">Myths spread by anti vaxers</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1495954-emerging-data-viral-vectors-treat-cancer.html\" id=\"thread_title_1495954\" itemprop=\"headline\">Emerging Data on viral vectors to treat cancer</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1492738-how-many-shots-once.html\" id=\"thread_title_1492738\" itemprop=\"headline\">How many shots at once?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1494338-i-gave-my-child-autism.html\" id=\"thread_title_1494338\" itemprop=\"headline\">\"I Gave My Child Autism\"</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1489346-2015-2016-flu-season.html\" id=\"thread_title_1489346\" itemprop=\"headline\">2015-2016 flu season</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1488874-jimmy-kimmel.html\" id=\"thread_title_1488874\" itemprop=\"headline\">Jimmy Kimmel</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1487850-have-any-your-kids-had-reaction.html\" id=\"thread_title_1487850\" itemprop=\"headline\">Have any of your kids had a reaction?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1481946-losing-friendships-over-vaxxing.html\" id=\"thread_title_1481946\" itemprop=\"headline\">Losing friendships over vaxxing?</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1486786-science-anti-vaccination.html\" id=\"thread_title_1486786\" itemprop=\"headline\">The Science of Anti-Vaccination</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1486226-ben-goldacre-vs-toronto-star.html\" id=\"thread_title_1486226\" itemprop=\"headline\">Ben Goldacre vs The Toronto Star</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1391631-article-anti-vaccination-activists-should-not-given-say-media.html\" id=\"thread_title_1391631\" itemprop=\"headline\">Article \"Anti-vaccination activists should not be given a say in the media\"  </a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1480394-do-your-research-pro-vax-blogs-journals.html\" id=\"thread_title_1480394\" itemprop=\"headline\">DO YOUR RESEARCH! Pro-vax Blogs and Journals</a>, <a class=\"thread_title_link\" href=\"http://www.mothering.com/forum/17507-vaccinating-schedule/1485026-chris-hayes-msnbc-oeall-ina-bob-sears.html\" id=\"thread_title_1485026\" itemprop=\"headline\">Chris Hayes MSNBC \\u201cAll In\\u201d Bob Sears</a>]\n"
     ]
    }
   ],
   "source": [
    "print links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:PythonWorkshop]",
   "language": "python",
   "name": "conda-env-PythonWorkshop-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
